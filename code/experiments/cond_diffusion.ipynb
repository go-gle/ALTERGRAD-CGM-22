{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VBY_UnKcVgnN"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hf8eajkIYoF9",
    "outputId": "21a18d57-7864-4a0a-8758-9f4e7a3906a9"
   },
   "outputs": [],
   "source": [
    "# For colab\n",
    "# !pip install torch_geometric\n",
    "# !pip install grakel\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_modules = '..'\n",
    "sys.path.append(path_to_modules)\n",
    "path_to_modules = '../src/baseline'\n",
    "sys.path.append(path_to_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "i0MdUUuKY-kH"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from src.cond_autoencoder import CondVariationalAutoEncoder\n",
    "from src.baseline.denoise_model import DenoiseNN, p_losses, sample\n",
    "from src.baseline.utils import linear_beta_schedule, construct_nx_from_adj, preprocess_dataset\n",
    "from src.utils import get_features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "C4j-jQkuZ7rr"
   },
   "outputs": [],
   "source": [
    "#deafault args\n",
    "lr = 1e-3\n",
    "dropout = 0\n",
    "batch_size = 256\n",
    "epochs_autoencoder = 200\n",
    "hidden_dim_encoder = 64\n",
    "hidden_dim_decoder =  256\n",
    "latent_dim = 32\n",
    "n_max_nodes = 50\n",
    "n_layers_encoder = 2\n",
    "n_layers_decoder =3\n",
    "spectral_emb_dim =10\n",
    "epochs_denoise = 100\n",
    "timesteps = 500\n",
    "hidden_dim_denoise = 512\n",
    "n_layers_denoise = 3\n",
    "dim_condition = 128\n",
    "n_condition = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GF7eP_3zZ3vf",
    "outputId": "d813b0d8-50ae-495a-aafe-a4f11fe99ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ../data/dataset_train.pt loaded from file\n",
      "Dataset ../data/dataset_valid.pt loaded from file\n",
      "Dataset ../data/dataset_test.pt loaded from file\n"
     ]
    }
   ],
   "source": [
    "# initialize VGAE model\n",
    "EARLY_STOP_ROUNS = 10\n",
    "MAX_NODES=50\n",
    "SPECTR_EMB_DIM = 10\n",
    "TRAIN_SIZE = 8000\n",
    "VAL_SIZE = 1000\n",
    "\n",
    "trainset = preprocess_dataset(\"train\", MAX_NODES, SPECTR_EMB_DIM)\n",
    "validset = preprocess_dataset(\"valid\", MAX_NODES, SPECTR_EMB_DIM)\n",
    "testset = preprocess_dataset(\"test\", MAX_NODES, SPECTR_EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WYeavBFugNNo"
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(spectral_emb_dim,\n",
    "                      hidden_dim_encoder,\n",
    "                      hidden_dim_decoder,\n",
    "                      latent_dim,\n",
    "                      n_layers_encoder,\n",
    "                      n_layers_decoder,\n",
    "                      lr,\n",
    "                      train_loader,\n",
    "                      val_loader,\n",
    "                      epochs_autoencoder=300,\n",
    "                      ):\n",
    "  autoencoder = CondVariationalAutoEncoder(spectral_emb_dim+1, hidden_dim_encoder, hidden_dim_decoder, latent_dim, n_layers_encoder, n_layers_decoder, n_max_nodes=MAX_NODES).to(DEVICE)\n",
    "  optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, threshold=5e-3, min_lr=5e-6)\n",
    "  early_stopping_counts = 0\n",
    "\n",
    "  best_val_loss = np.inf\n",
    "  for epoch in range(1, epochs_autoencoder + 1):\n",
    "      autoencoder.train()\n",
    "      train_loss_all = 0\n",
    "      train_count = 0\n",
    "      train_loss_all_recon = 0\n",
    "      train_loss_all_kld = 0\n",
    "      cnt_train=0\n",
    "\n",
    "      for data in train_loader:\n",
    "          data = data.to(DEVICE)\n",
    "          optimizer.zero_grad()\n",
    "          loss, recon, kld  = autoencoder.loss_function(data, data.stats)\n",
    "          train_loss_all_recon += recon.item()\n",
    "          train_loss_all_kld += kld.item()\n",
    "          cnt_train+=1\n",
    "          loss.backward()\n",
    "          train_loss_all += loss.item()\n",
    "          train_count += torch.max(data.batch)+1\n",
    "          optimizer.step()\n",
    "\n",
    "      autoencoder.eval()\n",
    "      val_loss_all = 0\n",
    "      val_count = 0\n",
    "      cnt_val = 0\n",
    "      val_loss_all_recon = 0\n",
    "      val_loss_all_kld = 0\n",
    "\n",
    "      for data in val_loader:\n",
    "          data = data.to(DEVICE)\n",
    "          loss, recon, kld  = autoencoder.loss_function(data, data.stats)\n",
    "          val_loss_all_recon += recon.item()\n",
    "          val_loss_all_kld += kld.item()\n",
    "          val_loss_all += loss.item()\n",
    "          cnt_val+=1\n",
    "          val_count += torch.max(data.batch)+1\n",
    "\n",
    "      # if epoch % 1 == 0:\n",
    "      #     print('Epoch: {:04d}, Train Loss: {:.5f}, Train Reconstruction Loss: {:.2f}, Train KLD Loss: {:.2f}, Val Loss: {:.5f}, Val Reconstruction Loss: {:.2f}, Val KLD Loss: {:.2f}'.format(epoch, train_loss_all/TRAIN_SIZE, train_loss_all_recon/TRAIN_SIZE, train_loss_all_kld/TRAIN_SIZE, val_loss_all/VAL_SIZE, val_loss_all_recon/VAL_SIZE, val_loss_all_kld/VAL_SIZE))\n",
    "\n",
    "      scheduler.step(val_loss_all)\n",
    "      early_stopping_counts += 1\n",
    "\n",
    "      if best_val_loss >= val_loss_all:\n",
    "          best_val_loss = val_loss_all\n",
    "          torch.save({\n",
    "              'state_dict': autoencoder.state_dict(),\n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "          }, 'autoencoder.pth.tar')\n",
    "          early_stopping_counts = 0\n",
    "\n",
    "\n",
    "      if early_stopping_counts == EARLY_STOP_ROUNS:\n",
    "        break\n",
    "\n",
    "  checkpoint = torch.load('autoencoder.pth.tar')\n",
    "  autoencoder.load_state_dict(checkpoint['state_dict'])\n",
    "  return autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bj7kV2PKg-Fy"
   },
   "outputs": [],
   "source": [
    "def train_denoiser(\n",
    "    n_layers_denoise,\n",
    "    timesteps,\n",
    "    n_condition,\n",
    "    dim_condition,\n",
    "    latent_dim,\n",
    "    hidden_dim_denoise,\n",
    "    autoencoder,\n",
    "    epochs_denoise,\n",
    "    lr,\n",
    "    beta_fn = linear_beta_schedule,\n",
    "):\n",
    "\n",
    "  betas = beta_fn(timesteps=timesteps)\n",
    "  alphas = 1. - betas\n",
    "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "  alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "  sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "  sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "  sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "  posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "  denoise_model = DenoiseNN(input_dim=latent_dim, hidden_dim=hidden_dim_denoise, n_layers=n_layers_denoise, n_cond=n_condition, d_cond=dim_condition).to(DEVICE)\n",
    "  optimizer = torch.optim.Adam(denoise_model.parameters(), lr=lr)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, threshold=5e-3, min_lr=5e-6)\n",
    "\n",
    "  best_val_loss = np.inf\n",
    "  early_stopping_counts = 0\n",
    "\n",
    "  for epoch in range(1, epochs_denoise+1):\n",
    "      denoise_model.train()\n",
    "      train_loss_all = 0\n",
    "      train_count = 0\n",
    "      for data in train_loader:\n",
    "          data = data.to(DEVICE)\n",
    "          optimizer.zero_grad()\n",
    "          x_g = autoencoder.encode(data, data.stats)\n",
    "          t = torch.randint(0, timesteps, (x_g.size(0),), device=DEVICE).long()\n",
    "          loss = p_losses(denoise_model, x_g, t, data.stats, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, loss_type=\"huber\")\n",
    "          loss.backward()\n",
    "          train_loss_all += x_g.size(0) * loss.item()\n",
    "          train_count += x_g.size(0)\n",
    "          optimizer.step()\n",
    "\n",
    "      denoise_model.eval()\n",
    "      val_loss_all = 0\n",
    "      val_count = 0\n",
    "      for data in val_loader:\n",
    "          data = data.to(DEVICE)\n",
    "          x_g = autoencoder.encode(data, data.stats)\n",
    "          t = torch.randint(0, timesteps, (x_g.size(0),), device=DEVICE).long()\n",
    "          loss = p_losses(denoise_model, x_g, t, data.stats, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, loss_type=\"huber\")\n",
    "          val_loss_all += x_g.size(0) * loss.item()\n",
    "          val_count += x_g.size(0)\n",
    "\n",
    "      scheduler.step(val_loss_all)\n",
    "      early_stopping_counts += 1\n",
    "      # if epoch % 5 == 0:\n",
    "      #   print('Epoch: {:04d}, Train Loss: {:.5f}, Val Loss: {:.5f}'.format(epoch, train_loss_all/TRAIN_SIZE, val_loss_all/VAL_SIZE))\n",
    "\n",
    "      if best_val_loss >= val_loss_all:\n",
    "          best_val_loss = val_loss_all\n",
    "          torch.save({\n",
    "              'state_dict': denoise_model.state_dict(),\n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "          }, 'denoise_model.pth.tar')\n",
    "          early_stopping_counts = 0\n",
    "\n",
    "      if early_stopping_counts == EARLY_STOP_ROUNS:\n",
    "        # print('early_stopping', best_val_loss, scheduler.get_last_lr())\n",
    "        break\n",
    "\n",
    "  checkpoint = torch.load('denoise_model.pth.tar')\n",
    "  denoise_model.load_state_dict(checkpoint['state_dict'])\n",
    "  denoise_model.eval()\n",
    "  return denoise_model, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pipeline and eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def eval(loader, autoencoder, denoiser, betas):\n",
    "  targets = []\n",
    "  preds = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in tqdm(loader):\n",
    "      data = data.to(DEVICE)\n",
    "      stat = data.stats\n",
    "      targets.append(stat.cpu().numpy())\n",
    "      bs = stat.size(0)\n",
    "      samples = sample(denoiser, stat, latent_dim=latent_dim, timesteps=timesteps, betas=betas, batch_size=bs)\n",
    "      x_sample = samples[-1]\n",
    "      adj = autoencoder.decode_mu(x_sample, stat)\n",
    "      for i in range(bs):\n",
    "        Gs_generated = construct_nx_from_adj(adj[i,:,:].detach().cpu().numpy())\n",
    "        preds.append(get_features(Gs_generated))\n",
    "\n",
    "  preds = np.array(preds)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  y_test_scaled = scaler.fit_transform(targets)\n",
    "  y_pred_scaled = scaler.transform(preds)\n",
    "  mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
    "  return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXEgcz6rRCMV",
    "outputId": "0253bb49-0bb0-415b-f2f4-8607eca7f89f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:37<00:00,  9.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34078903035047065"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "encoder = train_autoencoder(SPECTR_EMB_DIM,\n",
    "                          hidden_dim_encoder,\n",
    "                          hidden_dim_decoder,\n",
    "                          latent_dim,\n",
    "                          n_layers_encoder,\n",
    "                          n_layers_decoder,\n",
    "                          lr,\n",
    "                          train_loader,\n",
    "                          val_loader)\n",
    "denoiser, betas = train_denoiser(\n",
    "    n_layers_denoise,\n",
    "    timesteps,\n",
    "    7,\n",
    "    dim_condition,\n",
    "    latent_dim,\n",
    "    hidden_dim_denoise,\n",
    "    autoencoder=encoder,\n",
    "    epochs_denoise=300,\n",
    "    lr=lr,\n",
    "    beta_fn = linear_beta_schedule,\n",
    ")\n",
    "mae = eval(val_loader, encoder, denoiser, betas)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW9LHond_Krx",
    "outputId": "d78a60b5-b6b0-4a60-d0f5-0f1a2360a94b"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "n_condition = 7\n",
    "\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "with open(\"output.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header\n",
    "    writer.writerow([\"graph_id\", \"edge_list\"])\n",
    "    for k, data in enumerate(test_loader):\n",
    "        data = data.to(DEVICE)\n",
    "\n",
    "        stat = data.stats\n",
    "        bs = stat.size(0)\n",
    "\n",
    "        graph_ids = data.filename\n",
    "\n",
    "        samples = sample(denoiser, data.stats, latent_dim=latent_dim, timesteps=timesteps, betas=betas, batch_size=bs)\n",
    "        x_sample = samples[-1]\n",
    "        adj = encoder.decode_mu(x_sample, stat)\n",
    "        stat_d = torch.reshape(stat, (-1, n_condition))\n",
    "\n",
    "\n",
    "        for i in range(stat.size(0)):\n",
    "            stat_x = stat_d[i]\n",
    "\n",
    "            Gs_generated = construct_nx_from_adj(adj[i,:,:].detach().cpu().numpy())\n",
    "            stat_x = stat_x.detach().cpu().numpy()\n",
    "\n",
    "            # Define a graph ID\n",
    "            graph_id = graph_ids[i]\n",
    "\n",
    "            # Convert the edge list to a single string\n",
    "            edge_list_text = \", \".join([f\"({u}, {v})\" for u, v in Gs_generated.edges()])\n",
    "            # Write the graph ID and the full edge list as a single row\n",
    "            writer.writerow([graph_id, edge_list_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyte5z4UApxB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
