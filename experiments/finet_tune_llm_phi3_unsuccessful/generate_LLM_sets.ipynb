{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "\n",
    "from extract_feats import extract_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to write a very standard prompt, and merge everything together nicely so we can train a LoRA with as much ease as possible. TODO (train and val sets):\n",
    "- read every prompt, extract features, and reformulate it in a single, standard, short, clear prompt.\n",
    "- read every graph and structure it as in outpu.csv\n",
    "- do both in parallel so we can have access to the pairs as json lines inside the same file\n",
    "- create the dataset class and write it.\n",
    "\n",
    "TODO (test set):\n",
    "- convert the prompts in the format we chose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating the jsonl for data/train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:10<00:00, 738.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating the jsonl for data/valid set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 774.13it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "for set_ in ['train', 'valid']:\n",
    "    path = os.path.join(data_path, set_)\n",
    "    graphs_path = os.path.join(path, 'graph/')\n",
    "    prompts_path = os.path.join(path, 'description/')\n",
    "    graphs = os.listdir(graphs_path)\n",
    "    print(f'generating the jsonl for {path} set...')\n",
    "    for graph in tqdm(graphs):\n",
    "        #manage graphs\n",
    "        graph_file = os.path.join(graphs_path, graph)\n",
    "        if graph.endswith(\".graphml\"):\n",
    "            G = nx.read_graphml(graph_file)\n",
    "            G = nx.convert_node_labels_to_integers(G, ordering=\"sorted\")\n",
    "        else:\n",
    "            G = nx.read_edgelist(graph_file)    \n",
    "\n",
    "        #our answer\n",
    "        edges_str = str(list(G.edges()))[1:-1].replace(\"'\", \"\") \n",
    "\n",
    "        #manage prompts\n",
    "        fname = graph.split('.')[0]\n",
    "        txt_file = os.path.join(prompts_path, fname) + '.txt'\n",
    "        with open(txt_file, 'r') as f:\n",
    "            text = f.read()\n",
    "            features = extract_numbers(text)\n",
    "        #our prompt\n",
    "        prompt = f'Give the graph edgelist associated to the following features.-Number of nodes: {features[0]}-Number of edges: {features[1]}-Average degree: {features[2]}-Number of triangles: {features[3]}-Clustering coefficient: {features[4]}-Max k cores: {features[5]}-Number of communities: {features[6]}'\n",
    "\n",
    "        #write everything\n",
    "        formated_pair = '{\"prompt\": \"' + prompt + '\", \"answer\": \"' + edges_str + '\"}\\n'\n",
    "        with open(data_path + f'{set_}.jsonl', 'a') as f:\n",
    "            f.write(formated_pair)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 94070.11it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path =  os.path.join(data_path, 'test/')\n",
    "with open(test_path + 'test.txt', 'r') as ftest:\n",
    "    with open(data_path + 'test.jsonl', 'a') as new_f:\n",
    "        for text in tqdm(ftest.readlines()):\n",
    "            features = extract_numbers(text)\n",
    "            prompt = f'Give the graph edgelist associated to the following features.-Number of nodes: {features[0]}-Number of edges: {features[1]}-Average degree: {features[2]}-Number of triangles: {features[3]}-Clustering coefficient: {features[4]}-Max k cores: {features[5]}-Number of communities: {features[6]}'\n",
    "            formated = '{\"prompt\": \"' + prompt + '\"}\\n'\n",
    "            new_f.write(formated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make this a torch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatedFileDataset(Dataset):\n",
    "    def __init__(self, consolidated_file: str):\n",
    "        self.data: List[Dict[str, str]] = []\n",
    "        # Read and parse each line as JSON\n",
    "        with open(consolidated_file, 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                # Store the entire dictionary - more flexible\n",
    "                self.data.append(item)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, str]:\n",
    "        return self.data[idx]\n",
    "\n",
    "    def map(self, function, num_workers: int = None) -> 'ConcatenatedFileDataset':\n",
    "        \"\"\"Apply a function to all items in the dataset.\n",
    "        \n",
    "        Args:\n",
    "            function (Callable): The function to apply to each item\n",
    "            num_workers (int, optional): Number of workers for parallel processing\n",
    "        \n",
    "        Returns:\n",
    "            ConcatenatedFileDataset: A new dataset with transformed items\n",
    "        \"\"\"\n",
    "        # Create a new dataset instance\n",
    "        new_dataset = ConcatenatedFileDataset.__new__(ConcatenatedFileDataset)\n",
    "        \n",
    "        if num_workers and num_workers > 0:\n",
    "            # Parallel processing using Pool\n",
    "            with Pool(num_workers) as p:\n",
    "                new_dataset.data = list(p.map(function, self.data))\n",
    "        else:\n",
    "            # Sequential processing\n",
    "            new_dataset.data = [function(item) for item in self.data]\n",
    "            \n",
    "        return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ConcatenatedFileDataset('data/train.jsonl')\n",
    "torch.save(train, 'data/train.pt')\n",
    "test = ConcatenatedFileDataset('data/test.jsonl')\n",
    "torch.save(test, 'data/test.pt')\n",
    "valid = ConcatenatedFileDataset('data/valid.jsonl')\n",
    "torch.save(valid, 'data/valid.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the prints in the `demo_LLM_train.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
