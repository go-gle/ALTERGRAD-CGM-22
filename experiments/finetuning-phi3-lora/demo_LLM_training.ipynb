{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use an LLM to try and predict the edgelist of a graph just from the 7 features given to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install transformers accelerate flash_attn peft datasets bitsandbytes trl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModelForCausalLM, AutoPeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2 , cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "device_map = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(attn_implementation, ',', device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b422cc541e4340b8682f23efd6a797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model configuration\n",
    "model_id = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load model and tokenizer with explicit device map\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": torch.cuda.current_device()},  # This is the key change\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model.config.use_cache = False  # Required for gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Do you know what a graph edgelist is ? Answer in a single short sentence.\"}]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=32)\n",
    "\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onto preparting the way for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated the data in `generate_LLM_sets.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_LLM_sets import ConcatenatedFileDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63583/748146594.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train = torch.load('data/train.pt')\n",
      "/tmp/ipykernel_63583/748146594.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val = torch.load('data/valid.pt')\n"
     ]
    }
   ],
   "source": [
    "train = torch.load('data/train.pt')\n",
    "val = torch.load('data/valid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Give the graph edgelist associated to the following features.-Number of nodes: 15.0-Number of edges: 105.0-Average degree: 14.0-Number of triangles: 455.0-Clustering coefficient: 1.0-Max k cores: 14.0-Number of communities: 1.0', 'answer': '(0, 1), (0, 14), (0, 2), (0, 13), (0, 3), (0, 12), (0, 4), (0, 11), (0, 5), (0, 10), (0, 6), (0, 9), (0, 7), (0, 8), (1, 2), (1, 3), (1, 14), (1, 4), (1, 13), (1, 5), (1, 12), (1, 6), (1, 11), (1, 7), (1, 10), (1, 8), (1, 9), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (2, 3), (2, 4), (2, 5), (2, 6), (2, 13), (2, 7), (2, 12), (2, 8), (2, 11), (2, 9), (2, 10), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 12), (3, 10), (3, 11), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (10, 6), (10, 7), (10, 8), (10, 9), (6, 7), (6, 8), (6, 9), (9, 7), (9, 8), (7, 8)'}\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt to template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_column(row):\n",
    "    messages = [\n",
    "        {\"content\": row['prompt'], \"role\": \"user\"},\n",
    "        {\"content\": row['answer'], \"role\": \"assistant\"}\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    # Apply the chat template\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        row[\"messages\"], \n",
    "        add_generation_prompt=False, \n",
    "        tokenize=False\n",
    "    )\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors (or omit for lists)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"][0],  # Assuming batch size of 1 for simplicity\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"][0]  # Include this if needed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets with batched processing\n",
    "train_dataset = train.map(create_message_column)\n",
    "train_dataset = train_dataset.map(format_dataset_chatml)\n",
    "\n",
    "val_dataset = val.map(create_message_column)\n",
    "val_dataset = val_dataset.map(format_dataset_chatml)\n",
    "\n",
    "del train\n",
    "del val\n",
    "\n",
    "torch.save(train_dataset, 'data/train_dataset_tokenized.pt')\n",
    "torch.save(val_dataset, 'data/val_dataset_tokenized.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8765/1450608261.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_dataset = torch.load('data/train_dataset_tokenized.pt')\n",
      "/tmp/ipykernel_8765/1450608261.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_dataset = torch.load('data/val_dataset_tokenized.pt')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.load('data/train_dataset_tokenized.pt')\n",
    "val_dataset = torch.load('data/val_dataset_tokenized.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from training dataset:\n",
      "['input_ids', 'attention_mask']\n",
      "Length of first sample: 788\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample from training dataset:\")\n",
    "print(list(train_dataset[0].keys()))  # Should include 'input_ids'\n",
    "print(\"Length of first sample:\", len(train_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marceau/anaconda3/envs/altegrad/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi-3-mini-LoRA\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    max_steps=-1,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=1,  # Reduced from 8\n",
    "    gradient_accumulation_steps=16,   # Increased from 4\n",
    "    per_device_eval_batch_size=1,    # Reduced from 8\n",
    "    log_level=\"info\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    eval_steps=125,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=0,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is the most cursed thing known to man but hey it fixes all my problems and I have been fighting this notebook for way too long now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patched_forward(self, *args, **kwargs):\n",
    "    kwargs.pop(\"num_items_in_batch\", None)  # Ignore the extra argument\n",
    "    return PeftModelForCausalLM.forward(self, *args, **kwargs)  # Replace 'MyModelClass' with your model's actual class name\n",
    "\n",
    "from types import MethodType\n",
    "\n",
    "model.forward = MethodType(patched_forward, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8765/1639492350.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/home/marceau/anaconda3/envs/altegrad/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/marceau/anaconda3/envs/altegrad/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 8,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 8,912,896\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dfa74be98f43d2a25dfd167eedc198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/home/marceau/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.2236, 'grad_norm': 3.1693050861358643, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 8.739, 'grad_norm': 2.2582690715789795, 'learning_rate': 4e-05, 'epoch': 0.04}\n",
      "{'loss': 8.2193, 'grad_norm': 1.3784129619598389, 'learning_rate': 6e-05, 'epoch': 0.06}\n",
      "{'loss': 7.0056, 'grad_norm': 1.6875430345535278, 'learning_rate': 8e-05, 'epoch': 0.08}\n",
      "{'loss': 5.6848, 'grad_norm': 4.614770889282227, 'learning_rate': 0.0001, 'epoch': 0.1}\n",
      "{'loss': 4.2678, 'grad_norm': 1.5896835327148438, 'learning_rate': 9.987820251299122e-05, 'epoch': 0.12}\n",
      "{'loss': 3.9761, 'grad_norm': 0.992117702960968, 'learning_rate': 9.951340343707852e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5681, 'grad_norm': 1.0812567472457886, 'learning_rate': 9.890738003669029e-05, 'epoch': 0.16}\n",
      "{'loss': 3.6988, 'grad_norm': 1.237465739250183, 'learning_rate': 9.806308479691595e-05, 'epoch': 0.18}\n",
      "{'loss': 3.8249, 'grad_norm': 1.1787524223327637, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.2}\n",
      "{'loss': 3.7028, 'grad_norm': 1.5211529731750488, 'learning_rate': 9.567727288213005e-05, 'epoch': 0.22}\n",
      "{'loss': 3.7403, 'grad_norm': 1.2443379163742065, 'learning_rate': 9.414737964294636e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da0962786474697a9f08762ea5c8302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22255034744739532, 'eval_runtime': 877.9555, 'eval_samples_per_second': 1.139, 'eval_steps_per_second': 1.139, 'epoch': 0.25}\n",
      "{'loss': 3.5986, 'grad_norm': 1.624793529510498, 'learning_rate': 9.24024048078213e-05, 'epoch': 0.26}\n",
      "{'loss': 3.6561, 'grad_norm': 1.751072883605957, 'learning_rate': 9.045084971874738e-05, 'epoch': 0.28}\n",
      "{'loss': 3.6833, 'grad_norm': 1.7926079034805298, 'learning_rate': 8.83022221559489e-05, 'epoch': 0.3}\n",
      "{'loss': 3.1952, 'grad_norm': 1.2431806325912476, 'learning_rate': 8.596699001693255e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2922, 'grad_norm': 1.6546307802200317, 'learning_rate': 8.345653031794292e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4471, 'grad_norm': 1.3735625743865967, 'learning_rate': 8.07830737662829e-05, 'epoch': 0.36}\n",
      "{'loss': 3.5141, 'grad_norm': 1.6207785606384277, 'learning_rate': 7.795964517353735e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3031, 'grad_norm': 1.2811110019683838, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4332, 'grad_norm': 1.2855616807937622, 'learning_rate': 7.191855733945387e-05, 'epoch': 0.42}\n",
      "{'loss': 2.8676, 'grad_norm': 1.9748762845993042, 'learning_rate': 6.873032967079561e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1854, 'grad_norm': 1.9182724952697754, 'learning_rate': 6.545084971874738e-05, 'epoch': 0.46}\n",
      "{'loss': 3.4202, 'grad_norm': 2.0175411701202393, 'learning_rate': 6.209609477998338e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4652, 'grad_norm': 1.6332688331604004, 'learning_rate': 5.868240888334653e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbe4b1662d74abfbcf6326110619302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2084374725818634, 'eval_runtime': 877.7589, 'eval_samples_per_second': 1.139, 'eval_steps_per_second': 1.139, 'epoch': 0.5}\n",
      "{'loss': 3.3379, 'grad_norm': 2.233769178390503, 'learning_rate': 5.522642316338268e-05, 'epoch': 0.52}\n",
      "{'loss': 3.6246, 'grad_norm': 1.8723210096359253, 'learning_rate': 5.174497483512506e-05, 'epoch': 0.54}\n",
      "{'loss': 3.0862, 'grad_norm': 2.138923406600952, 'learning_rate': 4.825502516487497e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1581, 'grad_norm': 1.8332033157348633, 'learning_rate': 4.477357683661734e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3488, 'grad_norm': 1.2996113300323486, 'learning_rate': 4.131759111665349e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1478, 'grad_norm': 1.9925156831741333, 'learning_rate': 3.790390522001662e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3254, 'grad_norm': 2.1001808643341064, 'learning_rate': 3.4549150281252636e-05, 'epoch': 0.64}\n",
      "{'loss': 2.9687, 'grad_norm': 1.7294458150863647, 'learning_rate': 3.12696703292044e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3551, 'grad_norm': 2.266888380050659, 'learning_rate': 2.8081442660546125e-05, 'epoch': 0.68}\n",
      "{'loss': 3.0944, 'grad_norm': 2.541433334350586, 'learning_rate': 2.500000000000001e-05, 'epoch': 0.7}\n",
      "{'loss': 3.4179, 'grad_norm': 2.2792720794677734, 'learning_rate': 2.2040354826462668e-05, 'epoch': 0.72}\n",
      "{'loss': 3.0954, 'grad_norm': 1.8589732646942139, 'learning_rate': 1.9216926233717085e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df24034488545df846c0c8014a43cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20364397764205933, 'eval_runtime': 877.7728, 'eval_samples_per_second': 1.139, 'eval_steps_per_second': 1.139, 'epoch': 0.75}\n",
      "{'loss': 3.132, 'grad_norm': 2.599910020828247, 'learning_rate': 1.6543469682057106e-05, 'epoch': 0.76}\n",
      "{'loss': 2.9112, 'grad_norm': 1.8020118474960327, 'learning_rate': 1.4033009983067452e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1364, 'grad_norm': 2.038886785507202, 'learning_rate': 1.1697777844051105e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1151, 'grad_norm': 2.046433210372925, 'learning_rate': 9.549150281252633e-06, 'epoch': 0.82}\n",
      "{'loss': 2.9804, 'grad_norm': 2.28875994682312, 'learning_rate': 7.597595192178702e-06, 'epoch': 0.84}\n",
      "{'loss': 3.4798, 'grad_norm': 1.9011940956115723, 'learning_rate': 5.852620357053651e-06, 'epoch': 0.86}\n",
      "{'loss': 3.478, 'grad_norm': 2.946134328842163, 'learning_rate': 4.322727117869951e-06, 'epoch': 0.88}\n",
      "{'loss': 3.2395, 'grad_norm': 1.802404522895813, 'learning_rate': 3.0153689607045845e-06, 'epoch': 0.9}\n",
      "{'loss': 3.2249, 'grad_norm': 1.74669349193573, 'learning_rate': 1.9369152030840556e-06, 'epoch': 0.92}\n",
      "{'loss': 3.328, 'grad_norm': 1.9342437982559204, 'learning_rate': 1.0926199633097157e-06, 'epoch': 0.94}\n",
      "{'loss': 3.0573, 'grad_norm': 1.8740571737289429, 'learning_rate': 4.865965629214819e-07, 'epoch': 0.96}\n",
      "{'loss': 3.2902, 'grad_norm': 1.7172647714614868, 'learning_rate': 1.2179748700879012e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.302, 'grad_norm': 1.5325467586517334, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacd2ce2b2d44659b895971fe96f9228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi-3-mini-LoRA/checkpoint-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2023552656173706, 'eval_runtime': 877.7394, 'eval_samples_per_second': 1.139, 'eval_steps_per_second': 1.139, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/marceau/.cache/huggingface/hub/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-LoRA/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-LoRA/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./phi-3-mini-LoRA/checkpoint-500\n",
      "loading configuration file config.json from cache at /home/marceau/.cache/huggingface/hub/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-LoRA/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-LoRA/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./phi-3-mini-LoRA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 26707.2465, 'train_samples_per_second': 0.3, 'train_steps_per_second': 0.019, 'train_loss': 3.806950183868408, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/marceau/.cache/huggingface/hub/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-LoRA/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-LoRA/special_tokens_map.json\n",
      "loading configuration file config.json from cache at /home/marceau/.cache/huggingface/hub/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "/home/marceau/anaconda3/envs/altegrad/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "Configuration saved in ./merged_model/config.json\n",
      "Configuration saved in ./merged_model/generation_config.json\n",
      "Model weights saved in ./merged_model/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  # Added data collator\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model()\n",
    "\n",
    "# Optional: Save adapter only\n",
    "model.save_pretrained(\"./phi-3-mini-LoRA/final_adapter\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "save_path = \"./merged_model\"\n",
    "merged_model.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
