{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VBY_UnKcVgnN"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ccEhikSkYq1u"
   },
   "outputs": [],
   "source": [
    "# For colab\n",
    "\n",
    "# !pip install torch_geometric\n",
    "# !pip install grakel\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_modules = '..'\n",
    "sys.path.append(path_to_modules)\n",
    "path_to_modules = '../src/baseline'\n",
    "sys.path.append(path_to_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i0MdUUuKY-kH"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from src.cond_autoencoder import CondVariationalAutoEncoder\n",
    "from src.baseline.utils import construct_nx_from_adj, preprocess_dataset\n",
    "from src.utils import get_features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7quE96DqgOeE"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C4j-jQkuZ7rr"
   },
   "outputs": [],
   "source": [
    "#deafault args\n",
    "lr = 1e-3\n",
    "dropout = 0\n",
    "batch_size = 256\n",
    "epochs_autoencoder = 200\n",
    "hidden_dim_encoder = 64\n",
    "hidden_dim_decoder =  256\n",
    "latent_dim = 32\n",
    "n_max_nodes = 50\n",
    "n_layers_encoder = 2\n",
    "n_layers_decoder = 3\n",
    "spectral_emb_dim =10\n",
    "\n",
    "n_condition = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GF7eP_3zZ3vf",
    "outputId": "55635ff2-f579-49e6-b7d5-378041001b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ../data/dataset_train.pt loaded from file\n",
      "Dataset ../data/dataset_valid.pt loaded from file\n",
      "Dataset ../data/dataset_test.pt loaded from file\n"
     ]
    }
   ],
   "source": [
    "# initialize VGAE model\n",
    "EARLY_STOP_ROUNS = 10\n",
    "MAX_NODES=50\n",
    "SPECTR_EMB_DIM = 10\n",
    "TRAIN_SIZE = 8000\n",
    "VAL_SIZE = 1000\n",
    "\n",
    "trainset = preprocess_dataset(f\"train\", MAX_NODES, SPECTR_EMB_DIM)\n",
    "validset = preprocess_dataset(\"valid\", MAX_NODES, SPECTR_EMB_DIM)\n",
    "testset = preprocess_dataset(\"test\", MAX_NODES, SPECTR_EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WYeavBFugNNo"
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(spectral_emb_dim,\n",
    "                      hidden_dim_encoder,\n",
    "                      hidden_dim_decoder,\n",
    "                      latent_dim,\n",
    "                      n_layers_encoder,\n",
    "                      n_layers_decoder,\n",
    "                      lr,\n",
    "                      train_loader,\n",
    "                      val_loader,\n",
    "                      epochs_autoencoder=300,\n",
    "                      ):\n",
    "  autoencoder = CondVariationalAutoEncoder(spectral_emb_dim+1, hidden_dim_encoder, hidden_dim_decoder, latent_dim, n_layers_encoder, n_layers_decoder, n_max_nodes=MAX_NODES).to(DEVICE)\n",
    "  optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, threshold=5e-3, min_lr=5e-6)\n",
    "  early_stopping_counts = 0\n",
    "\n",
    "  best_val_loss = np.inf\n",
    "  for epoch in range(1, epochs_autoencoder + 1):\n",
    "      autoencoder.train()\n",
    "      train_loss_all = 0\n",
    "      train_count = 0\n",
    "      train_loss_all_recon = 0\n",
    "      train_loss_all_kld = 0\n",
    "      cnt_train=0\n",
    "\n",
    "      for data in train_loader:\n",
    "          data = data.to(DEVICE)\n",
    "          optimizer.zero_grad()\n",
    "          loss, recon, kld  = autoencoder.loss_function(data, data.stats)\n",
    "          train_loss_all_recon += recon.item()\n",
    "          train_loss_all_kld += kld.item()\n",
    "          cnt_train+=1\n",
    "          loss.backward()\n",
    "          train_loss_all += loss.item()\n",
    "          train_count += torch.max(data.batch)+1\n",
    "          optimizer.step()\n",
    "\n",
    "      autoencoder.eval()\n",
    "      val_loss_all = 0\n",
    "      val_count = 0\n",
    "      cnt_val = 0\n",
    "      val_loss_all_recon = 0\n",
    "      val_loss_all_kld = 0\n",
    "\n",
    "      for data in val_loader:\n",
    "          data = data.to(DEVICE)\n",
    "          loss, recon, kld  = autoencoder.loss_function(data, data.stats)\n",
    "          val_loss_all_recon += recon.item()\n",
    "          val_loss_all_kld += kld.item()\n",
    "          val_loss_all += loss.item()\n",
    "          cnt_val+=1\n",
    "          val_count += torch.max(data.batch)+1\n",
    "\n",
    "      # if epoch % 1 == 0:\n",
    "      #     print('Epoch: {:04d}, Train Loss: {:.5f}, Train Reconstruction Loss: {:.2f}, Train KLD Loss: {:.2f}, Val Loss: {:.5f}, Val Reconstruction Loss: {:.2f}, Val KLD Loss: {:.2f}'.format(epoch, train_loss_all/TRAIN_SIZE, train_loss_all_recon/TRAIN_SIZE, train_loss_all_kld/TRAIN_SIZE, val_loss_all/VAL_SIZE, val_loss_all_recon/VAL_SIZE, val_loss_all_kld/VAL_SIZE))\n",
    "\n",
    "      scheduler.step(val_loss_all)\n",
    "      early_stopping_counts += 1\n",
    "\n",
    "      if best_val_loss >= val_loss_all:\n",
    "          best_val_loss = val_loss_all\n",
    "          torch.save({\n",
    "              'state_dict': autoencoder.state_dict(),\n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "          }, 'autoencoder.pth.tar')\n",
    "          early_stopping_counts = 0\n",
    "\n",
    "\n",
    "      if early_stopping_counts == EARLY_STOP_ROUNS:\n",
    "        break\n",
    "\n",
    "  checkpoint = torch.load('autoencoder.pth.tar')\n",
    "  autoencoder.load_state_dict(checkpoint['state_dict'])\n",
    "  return autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t2k7udwS_17x"
   },
   "outputs": [],
   "source": [
    "# apply the pipeline and eval\n",
    "def eval(loader, autoencoder):\n",
    "  targets = []\n",
    "  preds = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      data = data.to(DEVICE)\n",
    "      stat = data.stats\n",
    "      targets.append(stat.cpu().numpy())\n",
    "      bs = stat.size(0)\n",
    "      x_sample = torch.randn(bs, latent_dim, device=DEVICE)\n",
    "      adj = autoencoder.decode_mu(x_sample, stat)\n",
    "      for i in range(bs):\n",
    "        Gs_generated = construct_nx_from_adj(adj[i,:,:].detach().cpu().numpy())\n",
    "        preds.append(get_features(Gs_generated))\n",
    "\n",
    "  preds = np.array(preds)\n",
    "  targets = np.concatenate(targets)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  y_test_scaled = scaler.fit_transform(targets)\n",
    "  y_pred_scaled = scaler.transform(preds)\n",
    "  mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
    "  return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WCev3vPORPwi"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "R86EIu-iU1Ts"
   },
   "outputs": [],
   "source": [
    "torch.set_warn_always(False)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZXEgcz6rRCMV"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "encoder = train_autoencoder(SPECTR_EMB_DIM,\n",
    "                          hidden_dim_encoder,\n",
    "                          hidden_dim_decoder,\n",
    "                          latent_dim,\n",
    "                          n_layers_encoder,\n",
    "                          n_layers_decoder,\n",
    "                          lr,\n",
    "                          train_loader,\n",
    "                          val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qr_UfhWweVDO",
    "outputId": "50a799e3-b48b-4b62-90d4-b08c622e4eab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38683516567940307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = eval(val_loader, encoder)\n",
    "\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCNLQ_t0odG2",
    "outputId": "ff797557-3584-4e42-b9ae-d980b0a3639e"
   },
   "outputs": [],
   "source": [
    "train_stat_np = []\n",
    "for data in trainset:\n",
    "  train_stat_np.append(data.stats.numpy())\n",
    "\n",
    "train_stat_np = np.concatenate(train_stat_np)\n",
    "scaler = StandardScaler()\n",
    "nn = NearestNeighbors(n_neighbors=1).fit(scaler.fit_transform(train_stat_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW9LHond_Krx",
    "outputId": "165b9059-036a-4a53-ebec-4b2766d43f27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test set: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "n_condition = 7\n",
    "\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "with open(\"knn_condVAE_test_stat.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header\n",
    "    writer.writerow([\"graph_id\", \"edge_list\"])\n",
    "    for k, data in enumerate(tqdm(test_loader, desc='Processing test set',)):\n",
    "        _, indices_neigh = nn.kneighbors(scaler.transform(data.stats.numpy()))\n",
    "        data = data.to(DEVICE)\n",
    "        graph_ids = data.filename\n",
    "\n",
    "        closest_neigh_from_train = []\n",
    "        for ind in indices_neigh.reshape(-1):\n",
    "          closest_neigh_from_train.append(trainset[ind])\n",
    "\n",
    "        loader_closest = DataLoader(closest_neigh_from_train, batch_size=batch_size, shuffle=False)\n",
    "        batch_closest = next(iter(loader_closest)).to(DEVICE)\n",
    "        stat = data.stats\n",
    "\n",
    "        x_g  = encoder.encoder(batch_closest, stat)\n",
    "        mu = encoder.fc_mu(x_g)\n",
    "        logvar = encoder.fc_logvar(x_g)\n",
    "        x_g = encoder.reparameterize(mu, logvar)\n",
    "        adj = encoder.decoder(x_g, stat)\n",
    "        stat_d = torch.reshape(data.stats, (-1, n_condition))\n",
    "\n",
    "        for i in range(stat.size(0)):\n",
    "            stat_x = stat_d[i]\n",
    "\n",
    "            Gs_generated = construct_nx_from_adj(adj[i,:,:].detach().cpu().numpy())\n",
    "            stat_x = stat_x.detach().cpu().numpy()\n",
    "\n",
    "            # Define a graph ID\n",
    "            graph_id = graph_ids[i]\n",
    "\n",
    "            # Convert the edge list to a single string\n",
    "            edge_list_text = \", \".join([f\"({u}, {v})\" for u, v in Gs_generated.edges()])\n",
    "            # Write the graph ID and the full edge list as a single row\n",
    "            writer.writerow([graph_id, edge_list_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyte5z4UApxB",
    "outputId": "4bddf1fa-b12e-4440-f238-d27999b8b6f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test set: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "n_condition = 7\n",
    "\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "with open(\"knn_condVAE_train_stat.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header\n",
    "    writer.writerow([\"graph_id\", \"edge_list\"])\n",
    "    for k, data in enumerate(tqdm(test_loader, desc='Processing test set',)):\n",
    "        _, indices_neigh = nn.kneighbors(scaler.transform(data.stats.numpy()))\n",
    "        data = data.to(DEVICE)\n",
    "        graph_ids = data.filename\n",
    "\n",
    "        closest_neigh_from_train = []\n",
    "        for ind in indices_neigh.reshape(-1):\n",
    "          closest_neigh_from_train.append(trainset[ind])\n",
    "\n",
    "        loader_closest = DataLoader(closest_neigh_from_train, batch_size=batch_size, shuffle=False)\n",
    "        batch_closest = next(iter(loader_closest)).to(DEVICE)\n",
    "        stat = batch_closest.stats\n",
    "\n",
    "        x_g  = encoder.encoder(batch_closest, stat)\n",
    "        mu = encoder.fc_mu(x_g)\n",
    "        logvar = encoder.fc_logvar(x_g)\n",
    "        x_g = encoder.reparameterize(mu, logvar)\n",
    "        adj = encoder.decoder(x_g, stat)\n",
    "        stat_d = torch.reshape(data.stats, (-1, n_condition))\n",
    "\n",
    "        for i in range(stat.size(0)):\n",
    "            stat_x = stat_d[i]\n",
    "\n",
    "            Gs_generated = construct_nx_from_adj(adj[i,:,:].detach().cpu().numpy())\n",
    "            stat_x = stat_x.detach().cpu().numpy()\n",
    "\n",
    "            # Define a graph ID\n",
    "            graph_id = graph_ids[i]\n",
    "\n",
    "            # Convert the edge list to a single string\n",
    "            edge_list_text = \", \".join([f\"({u}, {v})\" for u, v in Gs_generated.edges()])\n",
    "            # Write the graph ID and the full edge list as a single row\n",
    "            writer.writerow([graph_id, edge_list_text])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
